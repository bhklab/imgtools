{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Med-Imagetools: Transparent and Reproducible Medical Image Processing Pipelines in Python","text":""},{"location":"#med-imagetools-core-features","title":"Med-ImageTools core features","text":"<ul> <li>AutoPipeline CLI</li> <li><code>nnunet</code> nnU-Net compatibility mode</li> <li>Built-in train/test split for both normal/nnU-Net modes</li> <li><code>random_state</code> for reproducible seeds</li> <li>Region of interest (ROI) yaml dictionary intake for RTSTRUCT processing</li> <li>Markdown report output post-processing</li> <li><code>continue_processing</code> flag to continue autopipeline</li> <li><code>dry_run</code> flag to only crawl the dataset</li> </ul> <p>Med-Imagetools, a python package offers the perfect tool to transform messy medical dataset folders to deep learning ready format in few lines of code. It not only processes DICOMs consisting of different modalities (like CT, PET, RTDOSE and RTSTRUCTS), it also transforms them into deep learning ready subject based format taking the dependencies of these modalities into consideration.  </p>"},{"location":"#introduction","title":"Introduction","text":"<p>A medical dataset, typically contains multiple different types of scans for a single patient in a single study. As seen in the figure below, the different scans containing DICOM of different modalities are interdependent on each other. For making effective machine learning models, one ought to take different modalities into account.</p> <p></p> <p>Fig.1 - Different network topology for different studies of different patients</p> <p>Med-Imagetools is a unique tool, which focuses on subject based Machine learning. It crawls the dataset and makes a network by connecting different modalities present in the dataset. Based on the user defined modalities, med-imagetools, queries the graph and process the queried raw DICOMS. The processed DICOMS are saved as nrrds, which med-imagetools converts to torchio subject dataset and eventually torch dataloader for ML pipeline.</p> <p></p> <p>Fig.2 - Med-Imagetools AutoPipeline diagram</p>"},{"location":"#installing-med-imagetools","title":"Installing med-imagetools","text":"<pre><code>pip install med-imagetools\n</code></pre>"},{"location":"#recommended-create-new-conda-virtual-environment","title":"(recommended) Create new conda virtual environment","text":"<pre><code>conda create -n mit\nconda activate mit\npip install med-imagetools\n</code></pre>"},{"location":"#optional-install-in-development-mode","title":"(optional) Install in development mode","text":"<pre><code>conda create -n mit\nconda activate mit\npip install -e git+https://github.com/bhklab/med-imagetools.git\n</code></pre> <p>This will install the package in editable mode, so that the installed package will update when the code is changed.</p>"},{"location":"AutoPipeline/","title":"AutoPipeline Usage","text":"<p>To use AutoPipeline, follow the installation instructions found at https://github.com/bhklab/med-imagetools#installing-med-imagetools.</p>"},{"location":"AutoPipeline/#intro-to-autopipeline","title":"Intro to AutoPipeline","text":"<p>AutoPipeline will crawl and process any DICOM dataset. To run the most basic variation of the script, run the following command:</p> <pre><code>autopipeline INPUT_DIRECTORY OUTPUT_DIRECTORY --modalities MODALITY_LIST\n</code></pre> <p>Replace INPUT_DIRECTORY with the directory containing all your DICOM data, OUTPUT_DIRECTORY with the directory that you want the data to be outputted to.</p> <p>The <code>--modalities</code> option allows you to only process certain modalities that are present in the DICOM data. The available modalities are:</p> <ol> <li>CT</li> <li>MR</li> <li>RTSTRUCT</li> <li>PT    </li> <li>RTDOSE</li> </ol> <p>Set the modalities you want to use by separating each one with a comma. For example, to use CT and RTSTRUCT, run AutoPipeline with <code>--modalities CT,RTSTRUCT</code></p>"},{"location":"AutoPipeline/#autopipeline-flags","title":"AutoPipeline Flags","text":"<p>AutoPipeline comes with many built-in features to make your data processing easier:</p> <ol> <li> <p>Spacing</p> <p>The spacing for the output image. default = (1., 1., 0.). 0. spacing means maintaining the image's spacing as-is. Spacing of (0., 0., 0.,) will not resample any image.</p> <pre><code>--spacing [Tuple: (int,int,int)]\n</code></pre> </li> <li> <p>Parallel Job Execution</p> <p>The number of jobs to be run in parallel. Set -1 to use all cores. default = -1</p> <pre><code>--n_jobs [int]\n</code></pre> </li> <li> <p>Dataset Graph Visualization (not recommended for large datasets)</p> <p>Whether to visualize the entire dataset using PyViz.</p> <pre><code>--visualize [flag]\n</code></pre> </li> <li> <p>Continue Pipeline Processing</p> <p>Whether to continue the most recent run of AutoPipeline that terminated prematurely for that output directory. Will only work if the <code>.imgtools</code> directory was not deleted from previous run. Using this flag will retain the same flags and parameters carried over from the previous run.</p> <pre><code>--continue_processing [flag]\n</code></pre> </li> <li> <p>Processing Dry Run</p> <p>Whether to execute a dry run, only generating the .imgtools folder, which includes the crawled index.</p> <pre><code>--dry_run [flag]\n</code></pre> </li> <li> <p>Show Progress</p> <p>Whether to print AutoPipeline progress to the standard output.</p> <pre><code>--show_progress [flag]\n</code></pre> </li> <li> <p>Warning on Subject Processing Errors</p> <p>Whether to warn instead of error when processing subjects</p> <pre><code>--warn_on_error [flag]\n</code></pre> </li> <li> <p>Overwrite Existing Output Files</p> <p>Whether to overwrite existing file outputs</p> <pre><code>--overwrite [flag]\n</code></pre> </li> <li> <p>Update existing crawled index</p> <p>Whether to update existing crawled index</p> <pre><code>--update [flag]\n</code></pre> </li> </ol>"},{"location":"AutoPipeline/#flags-for-parsing-rtstruct-contoursregions-of-interest-roi","title":"Flags for parsing RTSTRUCT contours/regions of interest (ROI)","text":"<p>The contours can be selected by creating a YAML file to define a regular expression (regex), or list of potential contour names, or a combination of both. If none of the flags are set or the YAML file does not exist, the AutoPipeline will default to processing every contour.</p> <ol> <li> <p>Defining YAML file path for contours</p> <p>Whether to read a YAML file that defines regex or string options for contour names for regions of interest (ROI). By default, it will look for and read from <code>INPUT_DIRECTORY/roi_names.yaml</code></p> <pre><code>--read_yaml_label_names [flag]\n</code></pre> <p>Path to the above-mentioned YAML file. Path can be absolute or relative. default = \"\" (each ROI will have its own label index in dataset.json for nnUNet)</p> <pre><code>--roi_yaml_path [str]\n</code></pre> </li> <li> <p>Defining contour selection behaviour</p> <p>A typical ROI YAML file may look like this: <pre><code>GTV: GTV*\nLUNG:\n    - LUNG*\n    - LNUG\n    - POUMON*\nNODES:\n    - IL1\n    - IIL2\n    - IIIL3\n    - IVL4\n</code></pre></p> <p>By default, all ROIs that match any of the regex or strings will be saved as one label. For example, GTVn, GTVp, GTVfoo will be saved as GTV. However, this is not always the desirable behaviour. </p> <p>Only select the first matching regex/string</p> <p>The StructureSet iterates through the regex and string in the order it is written in the YAML. When this flag is set, once any contour matches the regex or string, the ROI search is interrupted and moves to the next ROI. This may be useful if you have a priority order of potentially matching contour names. </p> <pre><code>--roi_select_first [flag]\n</code></pre> <p>If a patient has  contours <code>[GTVp, LNUG, IL1, IVL4]</code>, with the above YAML file and <code>--roi_select_first</code> flag set, it will only process <code>[GTVp, LNUG, IL1]</code> contours as <code>[GTV, LUNG, NODES]</code>, respectively. </p> <p>Process each matching contour as a separate ROI</p> <p>Any matching contour will be saved separate with its contour name as a suffix to the ROI name. This will not apply to ROIs that only have one regex/string.</p> <p><pre><code>--roi_separate [flag]\n</code></pre> If a patient had contours <code>[GTVp, LNUG, IL1, IVL4]</code>, with the above YAML file and <code>--roi_sepearate</code> flag set, it will process the contours as <code>[GTV, LUNG_LNUG, NODES_IL1, NODES_IVL4]</code>, respectively. </p> </li> <li> <p>Ignore patients with no contours</p> <p>Ignore patients with no contours that match any of the defined regex or strings instead of throwing error. </p> <pre><code>--ignore_missing_regex [flag]\n</code></pre> </li> </ol>"},{"location":"AutoPipeline/#additional-nnunet-specific-flags","title":"Additional nnUNet-specific flags","text":"<ol> <li> <p>Format Output for nnUNet Training</p> <p>Whether to format output for nnUNet training. Modalities must be CT,RTSTRUCT or MR,RTSTRUCT. <code>--modalities CT,RTSTRUCT</code> or <code>--modalities MR,RTSTRUCT</code></p> <pre><code>--nnunet [flag]\n</code></pre> <pre><code>OUTPUT_DIRECTORY\n\u251c\u2500\u2500 nnUNet_preprocessed\n\u251c\u2500\u2500 nnUNet_raw_data_base\n\u2502   \u2514\u2500\u2500 nnUNet_raw_data\n\u2502       \u2514\u2500\u2500 Task500_HNSCC\n\u2502           \u251c\u2500\u2500 imagesTr\n\u2502           \u251c\u2500\u2500 imagesTs\n\u2502           \u251c\u2500\u2500 labelsTr\n\u2502           \u2514\u2500\u2500 labelsTs\n\u2514\u2500\u2500 nnUNet_trained_models\n</code></pre> </li> <li> <p>Training Size</p> <p>Training size of the train-test-split. default = 1.0 (all data will be in imagesTr/labelsTr)</p> <pre><code>--train_size [float]\n</code></pre> </li> <li> <p>Random State</p> <p>Random state for the train-test-split. Uses sklearn's train_test_split(). default = 42</p> <pre><code>--random_state [int]\n</code></pre> </li> <li> <p>Custom Train-Test-Split YAML</p> <p>Whether to use a custom train-test-split. Must be in a file found at <code>INPUT_DIRECTORY/custom_train_test_split.yaml</code>. All subjects not defined in this file will be randomly split to fill the defined value for <code>--train_size</code> (default = 1.0). File must conform to:</p> <pre><code>train:\n    - subject_1\n    - subject_2\n    ...\ntest:\n    - subject_1\n    - subject_2\n    ...\n</code></pre> <pre><code>--custom_train_test_split [flag]\n</code></pre> </li> </ol>"},{"location":"AutoPipeline/#additional-flags-for-nnunet-inference","title":"Additional flags for nnUNet Inference","text":"<ol> <li> <p>Format Output for nnUNet Inference</p> <p>Whether to format output for nnUNet Inference.</p> <pre><code>--nnunet_inference [flag]\n</code></pre> <pre><code>OUTPUT_DIRECTORY\n\u251c\u2500\u2500 0_subject1_0000.nii.gz\n\u2514\u2500\u2500 ...\n</code></pre> </li> <li> <p>Path to <code>dataset.json</code></p> <p>The path to the <code>dataset.json</code> file for nnUNet inference.</p> <pre><code>--dataset_json_path [str]\n</code></pre> <p>A dataset json file may look like this: <pre><code>{\n    \"modality\":{\n        \"0\": \"CT\"\n    }\n}\n</code></pre></p> </li> </ol>"},{"location":"nnUNet/","title":"Preparing Data for nnUNet","text":"<p>nnUNet repo can be found at: https://github.com/MIC-DKFZ/nnUNet</p>"},{"location":"nnUNet/#processing-dicom-data-with-med-imagetools","title":"Processing DICOM Data with Med-ImageTools","text":"<p>Ensure that you have followed the steps in https://github.com/bhklab/med-imagetools#installing-med-imagetools before proceeding.</p> <p>To convert your data from DICOM to NIfTI for training an nnUNet auto-segmentation model, run the following command:</p> <pre><code>autopipeline\\\n  [INPUT_DIRECTORY] \\\n  [OUTPUT_DIRECTORY] \\\n  --modalities CT,RTSTRUCT \\\n  --nnunet\n</code></pre> <p>Modalities can also be set to <code>--modalities MR,RTSTRUCT</code></p> <p>AutoPipeline offers many more options and features for you to customize your outputs: &lt;https://github.com/bhklab/med-imagetools/tree/main/README.md </p> <p>.</p>"},{"location":"nnUNet/#nnunet-preprocess-and-train","title":"nnUNet Preprocess and Train","text":""},{"location":"nnUNet/#one-step-preprocess-and-train","title":"One-Step Preprocess and Train","text":"<p>Med-ImageTools generates a file in your output folder called <code>nnunet_preprocess_and_train.sh</code> that combines all the commands needed for preprocessing and training your nnUNet model. Run that shell script to get a fully trained nnUNet model.</p> <p>Alternatively, you can go through each step individually as follows below:</p>"},{"location":"nnUNet/#nnunet-preprocessing","title":"nnUNet Preprocessing","text":"<p>Follow the instructions for setting up your paths for nnUNet: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/setting_up_paths.md</p> <p>Med-ImageTools generates the dataset.json that nnUNet requires in the output directory that you specify.</p> <p>The generated output directory structure will look something like:</p> <pre><code>OUTPUT_DIRECTORY\n\u251c\u2500\u2500 nnUNet_preprocessed\n\u251c\u2500\u2500 nnUNet_raw_data_base\n\u2502   \u2514\u2500\u2500 nnUNet_raw_data\n\u2502       \u2514\u2500\u2500 Task500_HNSCC\n\u2502           \u251c\u2500\u2500 nnunet_preprocess_and_train.sh\n\u2502           \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 nnUNet_trained_models\n</code></pre> <p>nnUNet requires that environment variables be set before any commands are executed. To temporarily set them, run the following:</p> <pre><code>export nnUNet_raw_data_base=\"/OUTPUT_DIRECTORY/nnUNet_raw_data_base\"\nexport nnUNet_preprocessed=\"/OUTPUT_DIRECTORY/nnUNet_preprocessed\"\nexport RESULTS_FOLDER=\"/OUTPUT_DIRECTORY/nnUNet_trained_models\"\n</code></pre> <p>To permanently set these environment variables, make sure that in your <code>~/.bashrc</code> file, these environment variables are set for nnUNet. The <code>nnUNet_preprocessed</code> and <code>nnUNet_trained_models</code> folders are generated as empty folders for you by Med-ImageTools. <code>nnUNet_raw_data_base</code> is populated with the required raw data files. Add this to the file:</p> <pre><code>export nnUNet_raw_data_base=\"/OUTPUT_DIRECTORY/nnUNet_raw_data_base\"\nexport nnUNet_preprocessed=\"/OUTPUT_DIRECTORY/nnUNet_preprocessed\"\nexport RESULTS_FOLDER=\"/OUTPUT_DIRECTORY/nnUNet_trained_models\"\n</code></pre> <p>Then, execute the command:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Too allow nnUNet to preprocess your data for training, run the following command. Set XXX to the ID that you want to preprocess. This is your task ID. For example, for Task500_HNSCC, the task ID is 500. Task IDs must be between 500 and 999, so Med-ImageTools can run 500 instances with the <code>--nnunet</code> flag in a single output folder.</p> <pre><code>nnUNet_plan_and_preprocess -t XXX --verify_dataset_integrity\n</code></pre>"},{"location":"nnUNet/#nnunet-training","title":"nnUNet Training","text":"<p>Once nnUNet has finished preprocessing, you may begin training your nnUNet model. To train your model, run the following command. Learn more about nnUNet's options here: https://github.com/MIC-DKFZ/nnUNet#model-training</p> <pre><code>nnUNet_train CONFIGURATION TRAINER_CLASS_NAME TASK_NAME_OR_ID FOLD\n</code></pre>"},{"location":"nnUNet/#nnunet-inference","title":"nnUNet Inference","text":"<p>For inference data, nnUNet requires data to be in a different output format. To run AutoPipeline for nnUNet inference, run the following command:</p> <p><pre><code>autopipeline\\\n  [INPUT_DIRECTORY] \\\n  [OUTPUT_DIRECTORY] \\\n  --modalities CT \\\n  --nnunet_inference \\\n  --dataset_json_path [DATASET_JSON_PATH]\n</code></pre> To execute this command AutoPipeline needs a json file with the image modality definitions.</p> <p>Modalities can also be set to <code>--modalities MR</code>.</p> <p>The directory structue will look like:</p> <pre><code>OUTPUT_DIRECTORY\n\u251c\u2500\u2500 0_subject1_0000.nii.gz\n\u2514\u2500\u2500 ...\n</code></pre> <p>To run inference, run the command:</p> <pre><code>nnUNet_predict -i INPUT_FOLDER -o OUTPUT_FOLDER -t TASK_NAME_OR_ID -m CONFIGURATION\n</code></pre> <p>In this case, the <code>INPUT_FOLDER</code> of nnUNet is the <code>OUTPUT_DIRECTORY</code> of Med-ImageTools.# Preparing Data for nnUNet</p> <p>nnUNet repo can be found at: https://github.com/MIC-DKFZ/nnUNet</p>"},{"location":"nnUNet/#processing-dicom-data-with-med-imagetools_1","title":"Processing DICOM Data with Med-ImageTools","text":"<p>Ensure that you have followed the steps in https://github.com/bhklab/med-imagetools#installing-med-imagetools before proceeding.</p> <p>To convert your data from DICOM to NIfTI for training an nnUNet auto-segmentation model, run the following command:</p> <pre><code>autopipeline\\\n  [INPUT_DIRECTORY] \\\n  [OUTPUT_DIRECTORY] \\\n  --modalities CT,RTSTRUCT \\\n  --nnunet\n</code></pre> <p>Modalities can also be set to <code>--modalities MR,RTSTRUCT</code></p> <p>AutoPipeline offers many more options and features for you to customize your outputs: https://github.com/bhklab/med-imagetools/imgtools/README.md.</p>"},{"location":"nnUNet/#nnunet-preprocess-and-train_1","title":"nnUNet Preprocess and Train","text":""},{"location":"nnUNet/#one-step-preprocess-and-train_1","title":"One-Step Preprocess and Train","text":"<p>Med-ImageTools generates a file in your output folder called <code>nnunet_preprocess_and_train.sh</code> that combines all the commands needed for preprocessing and training your nnUNet model. Run that shell script to get a fully trained nnUNet model.</p> <p>Alternatively, you can go through each step individually as follows below:</p>"},{"location":"nnUNet/#nnunet-preprocessing_1","title":"nnUNet Preprocessing","text":"<p>Follow the instructions for setting up your paths for nnUNet: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/setting_up_paths.md</p> <p>Med-ImageTools generates the dataset.json that nnUNet requires in the output directory that you specify.</p> <p>The generated output directory structure will look something like:</p> <pre><code>OUTPUT_DIRECTORY\n\u251c\u2500\u2500 nnUNet_preprocessed\n\u251c\u2500\u2500 nnUNet_raw_data_base\n\u2502   \u2514\u2500\u2500 nnUNet_raw_data\n\u2502       \u2514\u2500\u2500 Task500_HNSCC\n\u2502           \u251c\u2500\u2500 nnunet_preprocess_and_train.sh\n\u2502           \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 nnUNet_trained_models\n</code></pre> <p>nnUNet requires that environment variables be set before any commands are executed. To temporarily set them, run the following:</p> <pre><code>export nnUNet_raw_data_base=\"/OUTPUT_DIRECTORY/nnUNet_raw_data_base\"\nexport nnUNet_preprocessed=\"/OUTPUT_DIRECTORY/nnUNet_preprocessed\"\nexport RESULTS_FOLDER=\"/OUTPUT_DIRECTORY/nnUNet_trained_models\"\n</code></pre> <p>To permanently set these environment variables, make sure that in your <code>~/.bashrc</code> file, these environment variables are set for nnUNet. The <code>nnUNet_preprocessed</code> and <code>nnUNet_trained_models</code> folders are generated as empty folders for you by Med-ImageTools. <code>nnUNet_raw_data_base</code> is populated with the required raw data files. Add this to the file:</p> <pre><code>export nnUNet_raw_data_base=\"/OUTPUT_DIRECTORY/nnUNet_raw_data_base\"\nexport nnUNet_preprocessed=\"/OUTPUT_DIRECTORY/nnUNet_preprocessed\"\nexport RESULTS_FOLDER=\"/OUTPUT_DIRECTORY/nnUNet_trained_models\"\n</code></pre> <p>Then, execute the command:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Too allow nnUNet to preprocess your data for training, run the following command. Set XXX to the ID that you want to preprocess. This is your task ID. For example, for Task500_HNSCC, the task ID is 500. Task IDs must be between 500 and 999, so Med-ImageTools can run 500 instances with the <code>--nnunet</code> flag in a single output folder.</p> <pre><code>nnUNet_plan_and_preprocess -t XXX --verify_dataset_integrity\n</code></pre>"},{"location":"nnUNet/#nnunet-training_1","title":"nnUNet Training","text":"<p>Once nnUNet has finished preprocessing, you may begin training your nnUNet model. To train your model, run the following command. Learn more about nnUNet's options here: https://github.com/MIC-DKFZ/nnUNet#model-training</p> <pre><code>nnUNet_train CONFIGURATION TRAINER_CLASS_NAME TASK_NAME_OR_ID FOLD\n</code></pre>"},{"location":"nnUNet/#nnunet-inference_1","title":"nnUNet Inference","text":"<p>For inference data, nnUNet requires data to be in a different output format. To run AutoPipeline for nnUNet inference, run the following command:</p> <p><pre><code>autopipeline\\\n  [INPUT_DIRECTORY] \\\n  [OUTPUT_DIRECTORY] \\\n  --modalities CT \\\n  --nnunet_inference \\\n  --dataset_json_path [DATASET_JSON_PATH]\n</code></pre> To execute this command AutoPipeline needs a json file with the image modality definitions.</p> <p>Modalities can also be set to <code>--modalities MR</code>.</p> <p>The directory structue will look like:</p> <pre><code>OUTPUT_DIRECTORY\n\u251c\u2500\u2500 0_subject1_0000.nii.gz\n\u2514\u2500\u2500 ...\n</code></pre> <p>To run inference, run the command:</p> <pre><code>nnUNet_predict -i INPUT_FOLDER -o OUTPUT_FOLDER -t TASK_NAME_OR_ID -m CONFIGURATION\n</code></pre> <p>In this case, the <code>INPUT_FOLDER</code> of nnUNet is the <code>OUTPUT_DIRECTORY</code> of Med-ImageTools.</p>"},{"location":"reference/dicom-utils/find-dicoms/","title":"Find DICOMs","text":""},{"location":"reference/dicom-utils/find-dicoms/#imgtools.dicom.utils.find_dicoms","title":"imgtools.dicom.utils.find_dicoms","text":"<pre><code>find_dicoms(directory: Path, recursive: bool, check_header: bool, extension: Optional[str] = None) -&gt; List[Path]\n</code></pre> <p>Locate DICOM files in a specified directory.</p> <p>This function scans a directory for files matching the specified extension and validates them as DICOM files based on the provided options. It supports recursive search and optional header validation to confirm file validity.</p> <p>Parameters:</p> Name Type Description Default <code>Path</code> <p>The directory in which to search for DICOM files.</p> required <code>bool</code> <p>Whether to include subdirectories in the search.</p> <ul> <li>If <code>True</code>, recursively search all subdirectories.</li> <li>If <code>False</code>, search only the specified directory.</li> </ul> required <code>bool</code> <p>Whether to validate files by checking for a valid DICOM header.</p> <ul> <li>If <code>True</code>, perform DICOM header validation (slower but more accurate).</li> <li>If <code>False</code>, skip header validation and rely on extension.</li> </ul> required <code>str</code> <p>File extension to search for (e.g., \"dcm\"). If <code>None</code>, consider all files regardless of extension.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Path]</code> <p>A list of valid DICOM file paths found in the directory.</p> Notes <ul> <li>If <code>check_header</code> is enabled, the function checks each file for a valid   DICOM header, which may slow down the search process.</li> </ul> <p>Examples:</p> <p>Setup</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from imgtools.dicom.utils import find_dicoms\n</code></pre> <p>Find DICOM files recursively without header validation:</p> <pre><code>&gt;&gt;&gt; find_dicoms(Path('/data'), recursive=True, check_header=False)\n[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm'), PosixPath('/data/subdir/scan3.dcm')]\n</code></pre> <p>Suppose that <code>scan3.dcm</code> is not a valid DICOM file. Find DICOM files with header validation:</p> <pre><code>&gt;&gt;&gt; find_dicoms(Path('/data'), recursive=True, check_header=True)\n[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n</code></pre> <p>Find DICOM files without recursion:</p> <pre><code>&gt;&gt;&gt; find_dicoms(Path('/data'), recursive=False, check_header=False)\n[PosixPath('/data/scan1.dcm')]\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>def find_dicoms(\n\tdirectory: Path,\n\trecursive: bool,\n\tcheck_header: bool,\n\textension: Optional[str] = None,\n) -&gt; List[Path]:\n\t\"\"\"Locate DICOM files in a specified directory.\n\n\tThis function scans a directory for files matching the specified extension\n\tand validates them as DICOM files based on the provided options. It supports\n\trecursive search and optional header validation to confirm file validity.\n\n\tParameters\n\t----------\n\tdirectory : Path\n\t    The directory in which to search for DICOM files.\n\trecursive : bool\n\t    Whether to include subdirectories in the search.\n\n\t    - If `True`, recursively search all subdirectories.\n\t    - If `False`, search only the specified directory.\n\tcheck_header : bool\n\t    Whether to validate files by checking for a valid DICOM header.\n\n\t    - If `True`, perform DICOM header validation (slower but more accurate).\n\t    - If `False`, skip header validation and rely on extension.\n\n\textension : str, optional\n\t    File extension to search for (e.g., \"dcm\"). If `None`, consider all files\n\t    regardless of extension.\n\n\tReturns\n\t-------\n\tList[Path]\n\t    A list of valid DICOM file paths found in the directory.\n\n\tNotes\n\t-----\n\t- If `check_header` is enabled, the function checks each file for a valid\n\t  DICOM header, which may slow down the search process.\n\n\tExamples\n\t--------\n\tSetup\n\n\t&gt;&gt;&gt; from pathlib import Path\n\t&gt;&gt;&gt; from imgtools.dicom.utils import find_dicoms\n\n\tFind DICOM files recursively without header validation:\n\n\t&gt;&gt;&gt; find_dicoms(Path('/data'), recursive=True, check_header=False)\n\t[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm'), PosixPath('/data/subdir/scan3.dcm')]\n\n\tSuppose that `scan3.dcm` is not a valid DICOM file. Find DICOM files with header validation:\n\n\t&gt;&gt;&gt; find_dicoms(Path('/data'), recursive=True, check_header=True)\n\t[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n\n\tFind DICOM files without recursion:\n\t&gt;&gt;&gt; find_dicoms(Path('/data'), recursive=False, check_header=False)\n\t[PosixPath('/data/scan1.dcm')]\n\t\"\"\"\n\n\tpattern = f'*.{extension}' if extension else '*'\n\n\tglob_method = directory.rglob if recursive else directory.glob\n\n\tlogger.debug(\n\t\t'Looking for DICOM files',\n\t\tdirectory=directory,\n\t\trecursive=recursive,\n\t\tsearch_pattern=pattern,\n\t\tcheck_header=check_header,\n\t)\n\n\treturn [file.resolve() for file in glob_method(pattern) if _is_valid_dicom(file, check_header)]\n</code></pre>"},{"location":"reference/dicom-utils/find-dicoms/#imgtools.dicom.utils.find_dicoms(directory)","title":"<code>directory</code>","text":""},{"location":"reference/dicom-utils/find-dicoms/#imgtools.dicom.utils.find_dicoms(recursive)","title":"<code>recursive</code>","text":""},{"location":"reference/dicom-utils/find-dicoms/#imgtools.dicom.utils.find_dicoms(check_header)","title":"<code>check_header</code>","text":""},{"location":"reference/dicom-utils/find-dicoms/#imgtools.dicom.utils.find_dicoms(extension)","title":"<code>extension</code>","text":""},{"location":"reference/dicom-utils/tag-helpers/","title":"Tag Helpers","text":""},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.lookup_tag","title":"imgtools.dicom.utils.lookup_tag  <code>cached</code>","text":"<pre><code>lookup_tag(keyword: str, hex_format: bool = False) -&gt; Optional[str]\n</code></pre> <p>Lookup the tag for a given DICOM keyword.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The DICOM keyword to look up.</p> required <code>bool</code> <p>If True, return the tag in hexadecimal format (default is False).</p> <code>False</code> <p>Returns:</p> Type Description <code>str or None</code> <p>The DICOM tag as a string, or None if the keyword is invalid.</p> <p>Examples:</p> <p>Lookup a DICOM tag in decimal format:</p> <pre><code>&gt;&gt;&gt; lookup_tag('PatientID')\n'1048608'\n</code></pre> <p>Lookup a DICOM tag in hexadecimal format:</p> <pre><code>&gt;&gt;&gt; lookup_tag('PatientID', hex_format=True)\n'0x100020'\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>@functools.lru_cache(maxsize=1024)\ndef lookup_tag(keyword: str, hex_format: bool = False) -&gt; Optional[str]:\n\t\"\"\"\n\tLookup the tag for a given DICOM keyword.\n\n\tParameters\n\t----------\n\tkeyword : str\n\t    The DICOM keyword to look up.\n\thex_format : bool, optional\n\t    If True, return the tag in hexadecimal format (default is False).\n\n\tReturns\n\t-------\n\tstr or None\n\t    The DICOM tag as a string, or None if the keyword is invalid.\n\n\tExamples\n\t--------\n\n\tLookup a DICOM tag in decimal format:\n\n\t&gt;&gt;&gt; lookup_tag('PatientID')\n\t'1048608'\n\n\tLookup a DICOM tag in hexadecimal format:\n\n\t&gt;&gt;&gt; lookup_tag('PatientID', hex_format=True)\n\t'0x100020'\n\t\"\"\"\n\tif (tag := tag_for_keyword(keyword)) is None:\n\t\treturn None\n\treturn f'0x{tag:X}' if hex_format else str(tag)\n</code></pre>"},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.lookup_tag(keyword)","title":"<code>keyword</code>","text":""},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.lookup_tag(hex_format)","title":"<code>hex_format</code>","text":""},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.tag_exists","title":"imgtools.dicom.utils.tag_exists  <code>cached</code>","text":"<pre><code>tag_exists(keyword: str) -&gt; bool\n</code></pre> <p>Boolean check if a DICOM tag exists for a given keyword.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The DICOM keyword to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the tag exists, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tag_exists('PatientID')\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; tag_exists('InvalidKeyword')\nFalse\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>@functools.lru_cache(maxsize=1024)\ndef tag_exists(keyword: str) -&gt; bool:\n\t\"\"\"Boolean check if a DICOM tag exists for a given keyword.\n\n\tParameters\n\t----------\n\tkeyword : str\n\t    The DICOM keyword to check.\n\n\tReturns\n\t-------\n\tbool\n\t    True if the tag exists, False otherwise.\n\n\tExamples\n\t--------\n\n\t&gt;&gt;&gt; tag_exists('PatientID')\n\tTrue\n\n\t&gt;&gt;&gt; tag_exists('InvalidKeyword')\n\tFalse\n\t\"\"\"\n\treturn dictionary_has_tag(keyword)\n</code></pre>"},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.tag_exists(keyword)","title":"<code>keyword</code>","text":""},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.similar_tags","title":"imgtools.dicom.utils.similar_tags  <code>cached</code>","text":"<pre><code>similar_tags(keyword: str, n: int = 3, threshold: float = 0.6) -&gt; List[str]\n</code></pre> <p>Find similar DICOM tags for a given keyword.</p> <p>Useful for User Interface to suggest similar tags based on a misspelled keyword.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The keyword to search for similar tags.</p> required <code>int</code> <p>Maximum number of similar tags to return (default is 3).</p> <code>3</code> <code>float</code> <p>Minimum similarity ratio (default is 0.6).</p> <code>0.6</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of up to <code>n</code> similar DICOM tags.</p> <p>Examples:</p> <p>Find similar tags for a misspelled keyword:</p> <pre><code>&gt;&gt;&gt; similar_tags('PatinetID')\n['PatientID', 'PatientName', 'PatientBirthDate']\n</code></pre> <p>Adjust the number of results and threshold:</p> <pre><code>&gt;&gt;&gt; similar_tags('PatinetID', n=5, threshold=0.7)\n['PatientID', 'PatientName']\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>@functools.lru_cache(maxsize=1024)\ndef similar_tags(keyword: str, n: int = 3, threshold: float = 0.6) -&gt; List[str]:\n\t\"\"\"Find similar DICOM tags for a given keyword.\n\n\tUseful for User Interface to suggest similar tags based on a misspelled keyword.\n\n\tParameters\n\t----------\n\tkeyword : str\n\t    The keyword to search for similar tags.\n\tn : int, optional\n\t    Maximum number of similar tags to return (default is 3).\n\tthreshold : float, optional\n\t    Minimum similarity ratio (default is 0.6).\n\n\tReturns\n\t-------\n\tList[str]\n\t    A list of up to `n` similar DICOM tags.\n\n\tExamples\n\t--------\n\tFind similar tags for a misspelled keyword:\n\n\t&gt;&gt;&gt; similar_tags('PatinetID')\n\t['PatientID', 'PatientName', 'PatientBirthDate']\n\n\tAdjust the number of results and threshold:\n\n\t&gt;&gt;&gt; similar_tags('PatinetID', n=5, threshold=0.7)\n\t['PatientID', 'PatientName']\n\t\"\"\"\n\treturn difflib.get_close_matches(keyword, ALL_DICOM_TAGS, n, threshold)\n</code></pre>"},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.similar_tags(keyword)","title":"<code>keyword</code>","text":""},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.similar_tags(n)","title":"<code>n</code>","text":""},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.similar_tags(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/dicomsort/dicomsorter/","title":"DICOMSorter","text":""},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort","title":"imgtools.dicom.sort","text":"<p>Sorting DICOM Files by Specific Tags and Patterns.</p> <p>This module provides functionality to organize DICOM files into structured directories based on customizable target patterns.</p> <p>The target patterns allow metadata-driven file organization using placeholders for DICOM tags, enabling flexible and systematic storage.</p> Extended Summary <p>Target patterns define directory structures using placeholders, such as <code>%&lt;DICOMKey&gt;</code> and <code>{DICOMKey}</code>, which are resolved to their corresponding metadata values in the DICOM file.</p> <p>This approach ensures that files are organized based on their metadata, while retaining their original basenames. Files with identical metadata fields are placed in separate directories to preserve unique identifiers.</p> <p>Examples of target patterns:</p> <pre><code>- `%PatientID/%StudyID/{SeriesID}/`\n- `path/to_destination/%PatientID/images/%Modality/%SeriesInstanceUID/`\n</code></pre> <p>Important: Only the directory structure is modified during the sorting process. The basename of each file remains unchanged.</p> Notes <p>The module ensures that:</p> <ol> <li>Target patterns are resolved accurately based on the metadata in DICOM files.</li> <li>Files are placed in directories that reflect their resolved metadata fields.</li> <li>Original basenames are preserved to prevent unintended overwrites!</li> </ol> <p>Examples:</p> <p>Source file:</p> <pre><code>/source_dir/HN-CHUS-082/1-1.dcm\n</code></pre> <p>Target directory pattern:</p> <pre><code>./data/dicoms/%PatientID/Study-%StudyInstanceUID/Series-%SeriesInstanceUID/%Modality/\n</code></pre> <p>would result in the following structure for each file:</p> <pre><code>data/\n\u2514\u2500\u2500 dicoms/\n    \u2514\u2500\u2500 {PatientID}/\n        \u2514\u2500\u2500 Study-{StudyInstanceUID}/\n            \u2514\u2500\u2500 Series-{SeriesInstanceUID}/\n                \u2514\u2500\u2500 {Modality}/\n                    \u2514\u2500\u2500 1-1.dcm\n</code></pre> <p>And so the resolved path for the file would be:</p> <pre><code>./data/dicoms/HN-CHUS-082/Study-06980/Series-67882/RTSTRUCT/1-1.dcm\n</code></pre> <p>Here, the file is relocated into the resolved directory structure:</p> <pre><code>./data/dicoms/HN-CHUS-082/Study-06980/Series-67882/RTSTRUCT/\n</code></pre> <p>while the basename <code>1-1.dcm</code> remains unchanged.</p>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter","title":"imgtools.dicom.sort.DICOMSorter","text":"<pre><code>DICOMSorter(source_directory: Path, target_pattern: str, pattern_parser: Pattern = DEFAULT_PATTERN_PARSER)\n</code></pre> <p>A specialized implementation of the <code>SorterBase</code> for sorting DICOM files by metadata.</p> <p>This class resolves paths for DICOM files based on specified target patterns, using metadata extracted from the files. The filename of each source file is preserved during this process.</p> <p>Attributes:</p> Name Type Description <code>source_directory</code> <code>Path</code> <p>The directory containing the files to be sorted.</p> <code>logger</code> <code>Logger</code> <p>The instance logger bound with the source directory context.</p> <code>dicom_files</code> <code>list of Path</code> <p>The list of DICOM files found in the <code>source_directory</code>.</p> <code>format</code> <code>str</code> <p>The parsed format string with placeholders for DICOM tags.</p> <code>keys</code> <code>Set[str]</code> <p>DICOM tags extracted from the target pattern.</p> <code>invalid_keys</code> <code>Set[str]</code> <p>DICOM tags from the pattern that are invalid.</p> <p>Methods:</p> Name Description <code>execute</code> <p>Execute the file action on DICOM files.</p> <code>print_tree</code> <p>Display the pattern structure as a tree visualization.</p> <code>validate_keys</code> <p>Validate extracted keys. Subclasses should implement this method</p> Source code in <code>src/imgtools/dicom/sort/dicomsorter.py</code> <pre><code>def __init__(\n\tself,\n\tsource_directory: Path,\n\ttarget_pattern: str,\n\tpattern_parser: Pattern = DEFAULT_PATTERN_PARSER,\n) -&gt; None:\n\tsuper().__init__(\n\t\tsource_directory=source_directory,\n\t\ttarget_pattern=target_pattern,\n\t\tpattern_parser=pattern_parser,\n\t)\n\tself.logger.debug('All DICOM Keys are Valid in target pattern', keys=self.keys)\n</code></pre>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.format","title":"format  <code>property</code>","text":"<pre><code>format: str\n</code></pre> <p>Get the formatted pattern string.</p>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.invalid_keys","title":"invalid_keys  <code>property</code>","text":"<pre><code>invalid_keys: Set[str]\n</code></pre> <p>Get the set of invalid keys.</p> <p>Essentially, this will check <code>pydicom.dictionary_has_tag</code> for each key in the pattern and return the set of keys that are invalid.</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>The set of invalid keys.</p>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: Set[str]\n</code></pre> <p>Get the set of keys extracted from the pattern.</p>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.pattern_preview","title":"pattern_preview  <code>property</code>","text":"<pre><code>pattern_preview: str\n</code></pre> <p>Returns a human readable preview of the pattern.</p> <p>Useful for visualizing the pattern structure and can be highlighted using Rich Console.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; target_pattern = '%key1/%key2/%key3'\n&gt;&gt;&gt; pattern_preview = '{key1}/{key2}/{key3}'\n</code></pre>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.execute","title":"execute","text":"<pre><code>execute(action: FileAction = MOVE, overwrite: bool = False, dry_run: bool = False, num_workers: int = 1) -&gt; None\n</code></pre> <p>Execute the file action on DICOM files.</p> <p>Users are encouraged to use FileAction.HARDLINK for efficient storage and performance for large dataset, as well as protection against lost data.</p> <p>Using hard links can save disk space and improve performance by creating multiple directory entries (links) for a single file instead of duplicating the file content. This is particularly useful when working with large datasets, such as DICOM files, where storage efficiency is crucial.</p> <p>Parameters:</p> Name Type Description Default <code>FileAction</code> <pre><code>The action to apply to the DICOM files (e.g., move, copy).\n</code></pre> <code>FileAction.MOVE</code> <code>bool</code> <pre><code>If True, overwrite existing files at the destination.\n</code></pre> <code>False</code> <code>bool</code> <pre><code>If True, perform a dry run without making any changes.\n</code></pre> <code>False</code> <code>int</code> <pre><code>The number of worker threads to use for processing files.\n</code></pre> <code>1</code> Source code in <code>src/imgtools/dicom/sort/dicomsorter.py</code> <pre><code>def execute(\n\tself,\n\taction: FileAction = FileAction.MOVE,\n\toverwrite: bool = False,\n\tdry_run: bool = False,\n\tnum_workers: int = 1,\n) -&gt; None:\n\t\"\"\"Execute the file action on DICOM files.\n\n\tUsers are encouraged to use FileAction.HARDLINK for\n\tefficient storage and performance for large dataset, as well as\n\tprotection against lost data.\n\n\tUsing hard links can save disk space and improve performance by\n\tcreating multiple directory entries (links) for a single file\n\tinstead of duplicating the file content. This is particularly\n\tuseful when working with large datasets, such as DICOM files,\n\twhere storage efficiency is crucial.\n\n\tParameters\n\t----------\n\taction : FileAction, default: FileAction.MOVE\n\t        The action to apply to the DICOM files (e.g., move, copy).\n\toverwrite : bool, default: False\n\t        If True, overwrite existing files at the destination.\n\tdry_run : bool, default: False\n\t        If True, perform a dry run without making any changes.\n\tnum_workers : int, default: 1\n\t        The number of worker threads to use for processing files.\n\n\tRaises\n\t------\n\tValueError\n\t        If the provided action is not a valid FileAction.\n\t\"\"\"\n\tif not isinstance(action, FileAction):\n\t\taction = FileAction.validate(action)\n\n\tself.logger.debug(f'Mapping {len(self.dicom_files)} files to new paths')\n\n\t# Create a progress bar that can be used to track everything\n\twith self._progress_bar() as progress_bar:\n\t\t################################################################################\n\t\t# Resolve new paths\n\t\t################################################################################\n\t\tfile_map: Dict[Path, Path] = self._resolve_new_paths(\n\t\t\tprogress_bar=progress_bar, num_workers=num_workers\n\t\t)\n\tself.logger.info('Finished resolving paths')\n\n\t################################################################################\n\t# Check if any of the resolved paths are duplicates\n\t################################################################################\n\tfile_map = self._check_duplicates(file_map)\n\tself.logger.info('Finished checking for duplicates')\n\n\t################################################################################\n\t# Handle files\n\t################################################################################\n\tif dry_run:\n\t\tself._dry_run(file_map)\n\t\treturn\n\n\twith self._progress_bar() as progress_bar:\n\t\ttask_files = progress_bar.add_task('Handling files', total=len(file_map))\n\t\tnew_paths: List[Path | None] = []\n\t\twith ProcessPoolExecutor(max_workers=num_workers) as executor:\n\t\t\tfuture_to_file = {\n\t\t\t\texecutor.submit(\n\t\t\t\t\thandle_file, source_path, resolved_path, action, overwrite\n\t\t\t\t): source_path\n\t\t\t\tfor source_path, resolved_path in file_map.items()\n\t\t\t}\n\t\t\tfor future in as_completed(future_to_file):\n\t\t\t\ttry:\n\t\t\t\t\tresult = future.result()\n\t\t\t\t\tnew_paths.append(result)\n\t\t\t\t\tprogress_bar.update(task_files, advance=1)\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tself.logger.exception(\n\t\t\t\t\t\t'Failed to handle file',\n\t\t\t\t\t\texc_info=e,\n\t\t\t\t\t\tfile=future_to_file[future],\n\t\t\t\t\t)\n</code></pre>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.execute(action)","title":"<code>action</code>","text":""},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.execute(overwrite)","title":"<code>overwrite</code>","text":""},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.execute(dry_run)","title":"<code>dry_run</code>","text":""},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.execute(num_workers)","title":"<code>num_workers</code>","text":""},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.print_tree","title":"print_tree","text":"<pre><code>print_tree(base_dir: Path | None = None) -&gt; None\n</code></pre> <p>Display the pattern structure as a tree visualization.</p> Notes <p>This only prints the target pattern, parsed and formatted. Performing a dry-run execute will display more information.</p> Source code in <code>src/imgtools/dicom/sort/sorter_base.py</code> <pre><code>def print_tree(self, base_dir: Path | None = None) -&gt; None:\n\t\"\"\"\n\tDisplay the pattern structure as a tree visualization.\n\n\tNotes\n\t-----\n\tThis only prints the target pattern, parsed and formatted.\n\tPerforming a dry-run execute will display more information.\n\n\tRaises\n\t------\n\tSorterBaseError\n\t    If the tree visualization fails to generate.\n\t\"\"\"\n\ttry:\n\t\tbase_dir = base_dir or Path().cwd().resolve()\n\t\ttree = self._setup_tree(base_dir)\n\t\tself._generate_tree_structure(self.pattern_preview, tree)\n\t\tself._console.print(tree)\n\texcept Exception as e:\n\t\terrmsg = 'Failed to generate tree visualization.'\n\t\traise SorterBaseError(errmsg) from e\n</code></pre>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.validate_keys","title":"validate_keys","text":"<pre><code>validate_keys() -&gt; None\n</code></pre> <p>Validate extracted keys. Subclasses should implement this method to perform specific validations based on their context.</p> <p>Validate the DICOM keys in the target pattern.</p> <p>If any invalid keys are found, it suggests similar valid keys and raises an error.</p> Source code in <code>src/imgtools/dicom/sort/dicomsorter.py</code> <pre><code>def validate_keys(self) -&gt; None:\n\t\"\"\"Validate the DICOM keys in the target pattern.\n\n\tIf any invalid keys are found, it\n\tsuggests similar valid keys and raises an error.\n\t\"\"\"\n\tif not self.invalid_keys:\n\t\treturn\n\n\tfor key in sorted(self.invalid_keys):\n\t\t# TODO: keep this logic, but make the suggestion more user-friendly/readable\n\t\tsimilar = similar_tags(key)\n\t\tsuggestion = (\n\t\t\tf\"\\n\\tDid you mean: [bold green]{', '.join(similar)}[/bold green]?\"\n\t\t\tif similar\n\t\t\telse ' And [bold red]no similar keys[/bold red] found.'\n\t\t)\n\t\t_error = f'Invalid DICOM key: [bold red]{key}[/bold red].{suggestion}'\n\t\tself._console.print(f'{_error}')\n\tself._console.print(f'Parsed Path: `{self.pattern_preview}`')\n\terrmsg = 'Invalid DICOM Keys found.'\n\traise InvalidDICOMKeyError(errmsg)\n</code></pre>"},{"location":"reference/dicomsort/patternparser/","title":"PatternParser","text":""},{"location":"reference/dicomsort/patternparser/#imgtools.dicom.sort.parser","title":"imgtools.dicom.sort.parser","text":"<p>Parser module for extracting and validating placeholders from target patterns.</p> Summary <p>This module provides functionality to parse and validate sorting patterns with placeholders. Users can define custom regex patterns to extract keys from their sorting patterns.</p> Extended Summary <p>The <code>PatternParser</code> class allows users to define patterns with placeholders that can be replaced with actual values. The placeholders can be defined using custom regex patterns, making the parser flexible for various use cases.</p> <p>Examples:</p> <p>Setup:</p> <pre><code>&gt;&gt;&gt; import re\n&gt;&gt;&gt; from imgtools.dicom.sort.parser import PatternParser\n</code></pre> <p>Example 1: Suppose you want to parse a target pattern like <code>{Key1}-{Key2}</code> and replace the placeholders with values from a dictionary:</p> <pre><code>&gt;&gt;&gt; key_values = {'Key1': 'John', 'Key2': 'Doe'}\n</code></pre> <pre><code>&gt;&gt;&gt; pattern = '{Key1}-{Key2}'\n&gt;&gt;&gt; pattern_parser = re.compile(r'\\{(\\w+)\\}')\n&gt;&gt;&gt; parser = PatternParser(pattern, pattern_parser)\n&gt;&gt;&gt; formatted_pattern, keys = parser.parse()\n&gt;&gt;&gt; print(formatted_pattern)\n'%(Key1)s-%(Key2)s'\n&gt;&gt;&gt; print(keys)\n['Key1', 'Key2']\n</code></pre> <p>Now you can use the formatted pattern to replace the placeholders:</p> <pre><code>&gt;&gt;&gt; resolved_string = formatted_pattern % key_values\n&gt;&gt;&gt; print(resolved_string)\n'John-Doe'\n</code></pre> <p>Example 2: Suppose you want to parse a target pattern like <code>%&lt;Key1&gt; and {Key2}</code> and replace the placeholders with values from a dictionary:</p> <pre><code>&gt;&gt;&gt; key_values = {'Key1': 'Alice', 'Key2': 'Bob'}\n</code></pre> <pre><code>&gt;&gt;&gt; pattern = '%&lt;Key1&gt; and {Key2}'\n&gt;&gt;&gt; pattern_parser = re.compile(r'%&lt;(\\w+)&gt;|\\{(\\w+)\\}')\n&gt;&gt;&gt; parser = PatternParser(pattern, pattern_parser)\n&gt;&gt;&gt; formatted_pattern, keys = parser.parse()\n&gt;&gt;&gt; print(formatted_pattern)\n'%(Key1)s and %(Key2)s'\n&gt;&gt;&gt; print(keys)\n['Key1', 'Key2']\n</code></pre> <p>Now you can use the formatted pattern to replace the placeholders:</p> <pre><code>&gt;&gt;&gt; resolved_string = formatted_pattern % key_values\n&gt;&gt;&gt; print(resolved_string)\n'Alice and Bob'\n</code></pre> <p>Example 3: Suppose you want to parse a target pattern like <code>/path/to/{Key1}/and/{Key2}</code> and replace the placeholders with values from a dictionary:</p> <pre><code>&gt;&gt;&gt; key_values = {'Key1': 'folder1', 'Key2': 'folder2'}\n</code></pre> <pre><code>&gt;&gt;&gt; pattern = '/path/to/{Key1}/and/{Key2}'\n&gt;&gt;&gt; pattern_parser = re.compile(r'\\{(\\w+)\\}')\n&gt;&gt;&gt; parser = PatternParser(pattern, pattern_parser)\n&gt;&gt;&gt; formatted_pattern, keys = parser.parse()\n&gt;&gt;&gt; print(formatted_pattern)\n'/path/to/%(Key1)s/and/%(Key2)s'\n&gt;&gt;&gt; print(keys)\n['Key1', 'Key2']\n</code></pre> <p>Now you can use the formatted pattern to replace the placeholders:</p> <pre><code>&gt;&gt;&gt; resolved_string = formatted_pattern % key_values\n&gt;&gt;&gt; print(resolved_string)\n'/path/to/folder1/and/folder2'\n</code></pre>"},{"location":"reference/dicomsort/patternparser/#imgtools.dicom.sort.parser.PatternParser","title":"imgtools.dicom.sort.parser.PatternParser","text":"<pre><code>PatternParser(pattern: str, pattern_parser: Pattern)\n</code></pre> <p>A helper class to parse, validate, and sanitize sorting patterns.</p> <p>This class handles: - Pattern parsing and validation - Key extraction from patterns</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The pattern string to parse.</p> required <code>Pattern</code> <p>Custom regex pattern for parsing</p> required <p>Attributes:</p> Name Type Description <code>keys</code> <code>list of str</code> <p>Extracted keys from the pattern.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import re\n&gt;&gt;&gt; from imgtools.dicom.sort.parser import PatternParser\n</code></pre> <pre><code>&gt;&gt;&gt; key_values = {'Key1': 'Value1', 'Key2': 'Value2'}\n&gt;&gt;&gt; pattern = '{Key1}-{Key2}'\n&gt;&gt;&gt; pattern_parser = re.compile(r'\\{(\\w+)\\}')\n&gt;&gt;&gt; parser = PatternParser(pattern, pattern_parser)\n&gt;&gt;&gt; formatted_pattern, keys = parser.parse()\n&gt;&gt;&gt; print(formatted_pattern)\n'%(Key1)s-%(Key2)s'\n&gt;&gt;&gt; print(keys)\n['Key1', 'Key2']\n&gt;&gt;&gt; resolved_string = formatted_pattern % key_values\n&gt;&gt;&gt; print(resolved_string)\n'Value1-Value2'\n</code></pre> <p>Methods:</p> Name Description <code>parse</code> <p>Parse and validate the pattern.</p> Source code in <code>src/imgtools/dicom/sort/parser.py</code> <pre><code>def __init__(self, pattern: str, pattern_parser: Pattern) -&gt; None:\n\tassert isinstance(pattern, str) and pattern, 'Pattern must be a non-empty string.'\n\tself._pattern = pattern\n\tself._keys: List[str] = []\n\tassert isinstance(pattern_parser, Pattern), 'Pattern parser must be a regex pattern.'\n\tself._parser: Pattern = pattern_parser\n</code></pre>"},{"location":"reference/dicomsort/patternparser/#imgtools.dicom.sort.parser.PatternParser(pattern)","title":"<code>pattern</code>","text":""},{"location":"reference/dicomsort/patternparser/#imgtools.dicom.sort.parser.PatternParser(pattern_parser)","title":"<code>pattern_parser</code>","text":""},{"location":"reference/dicomsort/patternparser/#imgtools.dicom.sort.parser.PatternParser.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: List[str]\n</code></pre> <p>Get the list of extracted keys.</p>"},{"location":"reference/dicomsort/patternparser/#imgtools.dicom.sort.parser.PatternParser.parse","title":"parse","text":"<pre><code>parse() -&gt; Tuple[str, List[str]]\n</code></pre> <p>Parse and validate the pattern.</p> <p>Returns:</p> Type Description <code>Tuple[str, List[str]]</code> <p>The formatted pattern string and a list of extracted keys.</p> Source code in <code>src/imgtools/dicom/sort/parser.py</code> <pre><code>def parse(self) -&gt; Tuple[str, List[str]]:\n\t\"\"\"\n\tParse and validate the pattern.\n\n\tReturns\n\t-------\n\tTuple[str, List[str]]\n\t    The formatted pattern string and a list of extracted keys.\n\n\tRaises\n\t------\n\tInvalidPatternError\n\t    If the pattern contains no valid placeholders or is invalid.\n\t\"\"\"\n\n\tsanitized_pattern = self._pattern.strip()\n\tif not self._parser.search(sanitized_pattern):\n\t\terrmsg = f\"Pattern must contain placeholders matching '{self._parser.pattern}'.\"\n\t\traise InvalidPatternError(errmsg)\n\n\tformatted_pattern = self._parser.sub(self._replace_key, sanitized_pattern)\n\treturn formatted_pattern, self._keys\n</code></pre>"}]}